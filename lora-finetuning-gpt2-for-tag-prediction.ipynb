{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch transformers datasets peft accelerate scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:16:12.194938Z","iopub.execute_input":"2024-11-28T07:16:12.195882Z","iopub.status.idle":"2024-11-28T07:16:22.924524Z","shell.execute_reply.started":"2024-11-28T07:16:12.195833Z","shell.execute_reply":"2024-11-28T07:16:22.922817Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:16:22.926824Z","iopub.execute_input":"2024-11-28T07:16:22.927231Z","iopub.status.idle":"2024-11-28T07:16:26.837731Z","shell.execute_reply.started":"2024-11-28T07:16:22.927190Z","shell.execute_reply":"2024-11-28T07:16:26.836399Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# LOAD AND STURCTURE DATA\ndata = load_dataset(\"Abirate/english_quotes\")\n\n\ndef merge_columns(entry):\n    entry[\"prediction\"] = entry[\"quote\"] + \" ->: \" + str(entry[\"tags\"])\n    return entry\n\n\ndata['train'] = data['train'].map(merge_columns)\nprint(data['train']['prediction'][:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:17:25.911519Z","iopub.execute_input":"2024-11-28T07:17:25.914476Z","iopub.status.idle":"2024-11-28T07:17:27.077558Z","shell.execute_reply.started":"2024-11-28T07:17:25.914396Z","shell.execute_reply":"2024-11-28T07:17:27.076282Z"}},"outputs":[{"name":"stdout","text":"[\"“Be yourself; everyone else is already taken.” ->: ['be-yourself', 'gilbert-perreira', 'honesty', 'inspirational', 'misattributed-oscar-wilde', 'quote-investigator']\", \"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.” ->: ['best', 'life', 'love', 'mistakes', 'out-of-control', 'truth', 'worst']\", \"“Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.” ->: ['human-nature', 'humor', 'infinity', 'philosophy', 'science', 'stupidity', 'universe']\", \"“So many books, so little time.” ->: ['books', 'humor']\", \"“A room without books is like a body without a soul.” ->: ['books', 'simile', 'soul']\"]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndata = data.map(lambda samples: tokenizer(samples['prediction']), batched=True)\nprint(data[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:19:04.344007Z","iopub.execute_input":"2024-11-28T07:19:04.345256Z","iopub.status.idle":"2024-11-28T07:19:05.126626Z","shell.execute_reply.started":"2024-11-28T07:19:04.345208Z","shell.execute_reply":"2024-11-28T07:19:05.125427Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e45225e181054d56b9a91dd63ecaa176"}},"metadata":{}},{"name":"stdout","text":"{'quote': '“Be yourself; everyone else is already taken.”', 'author': 'Oscar Wilde', 'tags': ['be-yourself', 'gilbert-perreira', 'honesty', 'inspirational', 'misattributed-oscar-wilde', 'quote-investigator'], 'prediction': \"“Be yourself; everyone else is already taken.” ->: ['be-yourself', 'gilbert-perreira', 'honesty', 'inspirational', 'misattributed-oscar-wilde', 'quote-investigator']\", 'input_ids': [447, 250, 3856, 3511, 26, 2506, 2073, 318, 1541, 2077, 13, 447, 251, 4613, 25, 37250, 1350, 12, 14108, 944, 3256, 705, 37718, 4835, 12, 525, 260, 8704, 3256, 705, 24130, 9673, 3256, 705, 1040, 4063, 864, 3256, 705, 25413, 1078, 6169, 12, 418, 7718, 12, 21992, 68, 3256, 705, 22708, 12, 24859, 23823, 20520], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    \"gpt2\",\n    device_map='auto',\n)\n\n# FREEZE WEIGHTS\nfor param in model.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:21:45.076917Z","iopub.execute_input":"2024-11-28T07:21:45.079350Z","iopub.status.idle":"2024-11-28T07:21:46.008669Z","shell.execute_reply.started":"2024-11-28T07:21:45.079285Z","shell.execute_reply":"2024-11-28T07:21:46.007184Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79ccf4c180ae4073ae73a6ff8b4fabb2"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# LoRa\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:21:58.464801Z","iopub.execute_input":"2024-11-28T07:21:58.465309Z","iopub.status.idle":"2024-11-28T07:21:58.510730Z","shell.execute_reply.started":"2024-11-28T07:21:58.465268Z","shell.execute_reply":"2024-11-28T07:21:58.509028Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\nprint_trainable_parameters(model)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:22:05.393283Z","iopub.execute_input":"2024-11-28T07:22:05.393836Z","iopub.status.idle":"2024-11-28T07:22:05.406753Z","shell.execute_reply.started":"2024-11-28T07:22:05.393792Z","shell.execute_reply":"2024-11-28T07:22:05.404908Z"}},"outputs":[{"name":"stdout","text":"trainable params: 589824 || all params: 125029632 || trainable%: 0.4717473694555863\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# TRAINING\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=data['train'],\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        warmup_steps=100,\n        max_steps=500,\n        learning_rate=2e-4,\n        num_train_epochs=3,\n        logging_steps=1,\n        output_dir='outputs',\n        auto_find_batch_size=True\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\nmodel.config.use_cache = False\ntrainer.train()\n\ntorch.save(model.state_dict(), 'lora.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:22:42.183506Z","iopub.execute_input":"2024-11-28T07:22:42.184018Z","iopub.status.idle":"2024-11-28T09:01:46.327983Z","shell.execute_reply.started":"2024-11-28T07:22:42.183967Z","shell.execute_reply":"2024-11-28T09:01:46.325193Z"}},"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharicharan6991\u001b[0m (\u001b[33mharicharan6991-self\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113765922224654, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c500bdf42f3d4b3897dd989a3f89aa7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241128_072255-f9b079st</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/haricharan6991-self/huggingface/runs/f9b079st' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/haricharan6991-self/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/haricharan6991-self/huggingface' target=\"_blank\">https://wandb.ai/haricharan6991-self/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/haricharan6991-self/huggingface/runs/f9b079st' target=\"_blank\">https://wandb.ai/haricharan6991-self/huggingface/runs/f9b079st</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 1:38:39, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.652300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>4.493600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>4.246200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>4.412600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>4.122300</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>4.460700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>4.557600</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>4.564600</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>4.174600</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>4.519200</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>4.213400</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>4.062000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>4.051600</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>4.708900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>4.596200</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>4.433600</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>4.548400</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>4.176900</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>4.284900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>4.326900</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>4.639400</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>3.873600</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>4.484800</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>4.572000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>4.830200</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>4.669700</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>4.206900</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>4.443700</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>4.658800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>4.311000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>4.672300</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>4.456600</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>4.091500</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>4.184600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>3.977400</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>4.119300</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>4.248200</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>4.629700</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>4.577100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>4.466100</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>4.280800</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>4.195000</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>4.481900</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>4.171700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>4.018800</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>3.985200</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>4.166700</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>4.384700</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>4.142300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>4.082300</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>4.073100</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>4.154200</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>3.950100</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>3.969000</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>3.873200</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>3.939800</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>4.074100</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>4.047300</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>3.964800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>4.530800</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>4.265700</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>3.748300</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>3.702200</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>3.608200</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>3.735900</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>3.542200</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>3.788000</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>3.722300</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>3.858800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>3.793800</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>3.480400</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>3.739800</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>3.437300</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>3.554400</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>3.193200</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>3.077600</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>3.156300</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>3.327200</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>3.203700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.314800</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>3.089500</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>3.088000</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>3.091900</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>2.894900</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>3.288200</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>3.391600</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>3.047300</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>3.076300</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>3.325200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.888500</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>3.173000</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>3.364300</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>3.182600</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>3.084000</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>3.187500</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>3.245100</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>3.046500</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>3.097900</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>2.910900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.962400</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>2.993400</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>3.116500</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>2.998900</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>3.163000</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>2.850700</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>2.856700</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>3.334800</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>3.301700</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>2.823900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>3.039000</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>3.071400</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>2.909900</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>3.086600</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>3.069600</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>3.087900</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>3.079300</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>2.624700</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>2.841100</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>3.025700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>3.276200</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>2.828300</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>3.232100</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>2.967700</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>2.870000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>3.078900</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>3.000200</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>2.660100</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>3.006700</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>3.180300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>3.041500</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>2.802600</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>3.133900</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>2.830100</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>3.023400</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>2.923500</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>2.946700</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>2.839200</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>3.194400</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>2.964200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.704300</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>2.913100</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>3.166000</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>3.169500</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>2.877100</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>3.052800</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>3.214300</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>2.659400</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>2.880300</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>2.752000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.727300</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>2.833500</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>2.736300</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>2.606700</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>3.189800</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>2.846200</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>3.142200</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>2.969500</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>2.631700</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>2.532700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.613100</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>2.965500</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>3.177000</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>3.112100</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>2.789000</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>2.864100</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>2.733300</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>2.822600</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>2.904400</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>2.777900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>2.889900</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>2.945900</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>2.774500</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>2.860800</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>3.190900</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>3.097800</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>3.057800</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>3.035800</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>3.064200</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>2.595600</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.812500</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>2.709500</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>3.001600</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>2.826100</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>2.870900</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>2.830600</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>3.043700</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>3.056300</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>2.917300</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>2.883600</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.503100</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>2.859500</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>2.855800</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>3.268000</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>2.525300</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>3.051800</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>3.033700</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>2.996700</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>2.959700</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>2.788100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.016200</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>3.097800</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>3.105900</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>2.856700</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>2.952500</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>3.013900</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>2.743700</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>2.917100</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>2.936000</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>2.848500</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>2.861900</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>2.958900</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>2.621300</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>2.591100</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>2.710200</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>2.620400</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>2.657300</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>2.485500</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>2.502000</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>3.121200</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.935300</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>2.877400</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>2.887800</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>2.978100</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>2.556000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>2.747900</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>2.928900</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>2.912900</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>2.596300</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>2.918600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>3.188300</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>2.722700</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>2.919700</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>2.702100</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>2.660900</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>3.348000</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>2.701800</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>2.851900</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>2.787500</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>3.031400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.931800</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>2.940400</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>2.997600</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>2.645500</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>2.810800</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>3.131200</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>2.811400</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>2.771500</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>3.030200</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>2.701200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.891600</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>3.023300</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>2.896000</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>3.278900</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>2.890800</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>2.921100</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>3.037500</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>2.399400</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>2.873600</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>2.983400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.853700</td>\n    </tr>\n    <tr>\n      <td>261</td>\n      <td>2.612400</td>\n    </tr>\n    <tr>\n      <td>262</td>\n      <td>2.779000</td>\n    </tr>\n    <tr>\n      <td>263</td>\n      <td>2.850100</td>\n    </tr>\n    <tr>\n      <td>264</td>\n      <td>2.964200</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>2.915500</td>\n    </tr>\n    <tr>\n      <td>266</td>\n      <td>2.798800</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>3.098800</td>\n    </tr>\n    <tr>\n      <td>268</td>\n      <td>3.122000</td>\n    </tr>\n    <tr>\n      <td>269</td>\n      <td>3.136300</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>3.004700</td>\n    </tr>\n    <tr>\n      <td>271</td>\n      <td>2.398000</td>\n    </tr>\n    <tr>\n      <td>272</td>\n      <td>2.882100</td>\n    </tr>\n    <tr>\n      <td>273</td>\n      <td>2.942700</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>2.819100</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>2.785600</td>\n    </tr>\n    <tr>\n      <td>276</td>\n      <td>2.866200</td>\n    </tr>\n    <tr>\n      <td>277</td>\n      <td>2.978900</td>\n    </tr>\n    <tr>\n      <td>278</td>\n      <td>2.729300</td>\n    </tr>\n    <tr>\n      <td>279</td>\n      <td>2.830200</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.843300</td>\n    </tr>\n    <tr>\n      <td>281</td>\n      <td>2.697700</td>\n    </tr>\n    <tr>\n      <td>282</td>\n      <td>2.799300</td>\n    </tr>\n    <tr>\n      <td>283</td>\n      <td>2.872900</td>\n    </tr>\n    <tr>\n      <td>284</td>\n      <td>2.926000</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>3.158400</td>\n    </tr>\n    <tr>\n      <td>286</td>\n      <td>2.798200</td>\n    </tr>\n    <tr>\n      <td>287</td>\n      <td>3.024100</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>3.329800</td>\n    </tr>\n    <tr>\n      <td>289</td>\n      <td>3.059900</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>2.652500</td>\n    </tr>\n    <tr>\n      <td>291</td>\n      <td>2.910800</td>\n    </tr>\n    <tr>\n      <td>292</td>\n      <td>2.753000</td>\n    </tr>\n    <tr>\n      <td>293</td>\n      <td>3.089200</td>\n    </tr>\n    <tr>\n      <td>294</td>\n      <td>2.983900</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>2.804500</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>2.949100</td>\n    </tr>\n    <tr>\n      <td>297</td>\n      <td>2.648200</td>\n    </tr>\n    <tr>\n      <td>298</td>\n      <td>2.667000</td>\n    </tr>\n    <tr>\n      <td>299</td>\n      <td>2.777400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.805500</td>\n    </tr>\n    <tr>\n      <td>301</td>\n      <td>2.998000</td>\n    </tr>\n    <tr>\n      <td>302</td>\n      <td>2.857100</td>\n    </tr>\n    <tr>\n      <td>303</td>\n      <td>3.211700</td>\n    </tr>\n    <tr>\n      <td>304</td>\n      <td>2.671700</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>2.824200</td>\n    </tr>\n    <tr>\n      <td>306</td>\n      <td>2.732800</td>\n    </tr>\n    <tr>\n      <td>307</td>\n      <td>2.763900</td>\n    </tr>\n    <tr>\n      <td>308</td>\n      <td>2.669000</td>\n    </tr>\n    <tr>\n      <td>309</td>\n      <td>2.913300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>2.873900</td>\n    </tr>\n    <tr>\n      <td>311</td>\n      <td>2.842500</td>\n    </tr>\n    <tr>\n      <td>312</td>\n      <td>3.037400</td>\n    </tr>\n    <tr>\n      <td>313</td>\n      <td>3.005800</td>\n    </tr>\n    <tr>\n      <td>314</td>\n      <td>2.750700</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>2.542800</td>\n    </tr>\n    <tr>\n      <td>316</td>\n      <td>2.945200</td>\n    </tr>\n    <tr>\n      <td>317</td>\n      <td>3.171600</td>\n    </tr>\n    <tr>\n      <td>318</td>\n      <td>2.454200</td>\n    </tr>\n    <tr>\n      <td>319</td>\n      <td>2.877000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.997300</td>\n    </tr>\n    <tr>\n      <td>321</td>\n      <td>2.709700</td>\n    </tr>\n    <tr>\n      <td>322</td>\n      <td>2.616000</td>\n    </tr>\n    <tr>\n      <td>323</td>\n      <td>2.852500</td>\n    </tr>\n    <tr>\n      <td>324</td>\n      <td>2.809900</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>2.388700</td>\n    </tr>\n    <tr>\n      <td>326</td>\n      <td>2.990500</td>\n    </tr>\n    <tr>\n      <td>327</td>\n      <td>2.815500</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>2.881000</td>\n    </tr>\n    <tr>\n      <td>329</td>\n      <td>2.909900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>3.022400</td>\n    </tr>\n    <tr>\n      <td>331</td>\n      <td>3.120400</td>\n    </tr>\n    <tr>\n      <td>332</td>\n      <td>2.760900</td>\n    </tr>\n    <tr>\n      <td>333</td>\n      <td>2.735800</td>\n    </tr>\n    <tr>\n      <td>334</td>\n      <td>2.875400</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>2.433300</td>\n    </tr>\n    <tr>\n      <td>336</td>\n      <td>3.053000</td>\n    </tr>\n    <tr>\n      <td>337</td>\n      <td>2.977900</td>\n    </tr>\n    <tr>\n      <td>338</td>\n      <td>2.621500</td>\n    </tr>\n    <tr>\n      <td>339</td>\n      <td>3.166800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.725400</td>\n    </tr>\n    <tr>\n      <td>341</td>\n      <td>3.118400</td>\n    </tr>\n    <tr>\n      <td>342</td>\n      <td>2.645700</td>\n    </tr>\n    <tr>\n      <td>343</td>\n      <td>2.935500</td>\n    </tr>\n    <tr>\n      <td>344</td>\n      <td>2.878000</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>2.740900</td>\n    </tr>\n    <tr>\n      <td>346</td>\n      <td>2.783600</td>\n    </tr>\n    <tr>\n      <td>347</td>\n      <td>2.701700</td>\n    </tr>\n    <tr>\n      <td>348</td>\n      <td>2.839800</td>\n    </tr>\n    <tr>\n      <td>349</td>\n      <td>2.677000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.844500</td>\n    </tr>\n    <tr>\n      <td>351</td>\n      <td>2.817800</td>\n    </tr>\n    <tr>\n      <td>352</td>\n      <td>2.563500</td>\n    </tr>\n    <tr>\n      <td>353</td>\n      <td>2.738100</td>\n    </tr>\n    <tr>\n      <td>354</td>\n      <td>2.627100</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>2.732700</td>\n    </tr>\n    <tr>\n      <td>356</td>\n      <td>2.842300</td>\n    </tr>\n    <tr>\n      <td>357</td>\n      <td>2.710800</td>\n    </tr>\n    <tr>\n      <td>358</td>\n      <td>3.104200</td>\n    </tr>\n    <tr>\n      <td>359</td>\n      <td>2.769600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.898100</td>\n    </tr>\n    <tr>\n      <td>361</td>\n      <td>3.091600</td>\n    </tr>\n    <tr>\n      <td>362</td>\n      <td>2.887800</td>\n    </tr>\n    <tr>\n      <td>363</td>\n      <td>2.745300</td>\n    </tr>\n    <tr>\n      <td>364</td>\n      <td>2.690100</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>3.063000</td>\n    </tr>\n    <tr>\n      <td>366</td>\n      <td>3.141300</td>\n    </tr>\n    <tr>\n      <td>367</td>\n      <td>2.685500</td>\n    </tr>\n    <tr>\n      <td>368</td>\n      <td>2.851500</td>\n    </tr>\n    <tr>\n      <td>369</td>\n      <td>2.800700</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>2.844100</td>\n    </tr>\n    <tr>\n      <td>371</td>\n      <td>2.888800</td>\n    </tr>\n    <tr>\n      <td>372</td>\n      <td>2.902200</td>\n    </tr>\n    <tr>\n      <td>373</td>\n      <td>3.058600</td>\n    </tr>\n    <tr>\n      <td>374</td>\n      <td>2.951600</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>2.757700</td>\n    </tr>\n    <tr>\n      <td>376</td>\n      <td>2.631200</td>\n    </tr>\n    <tr>\n      <td>377</td>\n      <td>3.053000</td>\n    </tr>\n    <tr>\n      <td>378</td>\n      <td>2.606800</td>\n    </tr>\n    <tr>\n      <td>379</td>\n      <td>2.799900</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>2.729400</td>\n    </tr>\n    <tr>\n      <td>381</td>\n      <td>3.069800</td>\n    </tr>\n    <tr>\n      <td>382</td>\n      <td>2.815600</td>\n    </tr>\n    <tr>\n      <td>383</td>\n      <td>2.859100</td>\n    </tr>\n    <tr>\n      <td>384</td>\n      <td>2.884900</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>2.797500</td>\n    </tr>\n    <tr>\n      <td>386</td>\n      <td>2.913500</td>\n    </tr>\n    <tr>\n      <td>387</td>\n      <td>3.137300</td>\n    </tr>\n    <tr>\n      <td>388</td>\n      <td>2.828300</td>\n    </tr>\n    <tr>\n      <td>389</td>\n      <td>3.148600</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>3.190900</td>\n    </tr>\n    <tr>\n      <td>391</td>\n      <td>2.749400</td>\n    </tr>\n    <tr>\n      <td>392</td>\n      <td>2.699800</td>\n    </tr>\n    <tr>\n      <td>393</td>\n      <td>2.780200</td>\n    </tr>\n    <tr>\n      <td>394</td>\n      <td>2.718900</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>2.976800</td>\n    </tr>\n    <tr>\n      <td>396</td>\n      <td>2.783100</td>\n    </tr>\n    <tr>\n      <td>397</td>\n      <td>2.778700</td>\n    </tr>\n    <tr>\n      <td>398</td>\n      <td>2.693400</td>\n    </tr>\n    <tr>\n      <td>399</td>\n      <td>3.182900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.770700</td>\n    </tr>\n    <tr>\n      <td>401</td>\n      <td>2.491900</td>\n    </tr>\n    <tr>\n      <td>402</td>\n      <td>3.035100</td>\n    </tr>\n    <tr>\n      <td>403</td>\n      <td>2.742600</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>2.785400</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>2.653200</td>\n    </tr>\n    <tr>\n      <td>406</td>\n      <td>2.751200</td>\n    </tr>\n    <tr>\n      <td>407</td>\n      <td>2.911300</td>\n    </tr>\n    <tr>\n      <td>408</td>\n      <td>2.915400</td>\n    </tr>\n    <tr>\n      <td>409</td>\n      <td>2.530200</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>2.591100</td>\n    </tr>\n    <tr>\n      <td>411</td>\n      <td>2.796900</td>\n    </tr>\n    <tr>\n      <td>412</td>\n      <td>2.693000</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>2.569400</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>2.807800</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>2.811700</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>2.862300</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>2.794400</td>\n    </tr>\n    <tr>\n      <td>418</td>\n      <td>2.958700</td>\n    </tr>\n    <tr>\n      <td>419</td>\n      <td>3.214900</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>2.697200</td>\n    </tr>\n    <tr>\n      <td>421</td>\n      <td>2.883400</td>\n    </tr>\n    <tr>\n      <td>422</td>\n      <td>2.737200</td>\n    </tr>\n    <tr>\n      <td>423</td>\n      <td>2.613000</td>\n    </tr>\n    <tr>\n      <td>424</td>\n      <td>2.965900</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>3.131500</td>\n    </tr>\n    <tr>\n      <td>426</td>\n      <td>2.943600</td>\n    </tr>\n    <tr>\n      <td>427</td>\n      <td>2.878800</td>\n    </tr>\n    <tr>\n      <td>428</td>\n      <td>2.817500</td>\n    </tr>\n    <tr>\n      <td>429</td>\n      <td>3.103300</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>3.291900</td>\n    </tr>\n    <tr>\n      <td>431</td>\n      <td>2.725000</td>\n    </tr>\n    <tr>\n      <td>432</td>\n      <td>3.115000</td>\n    </tr>\n    <tr>\n      <td>433</td>\n      <td>2.583700</td>\n    </tr>\n    <tr>\n      <td>434</td>\n      <td>3.000500</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>2.937900</td>\n    </tr>\n    <tr>\n      <td>436</td>\n      <td>2.658900</td>\n    </tr>\n    <tr>\n      <td>437</td>\n      <td>3.025300</td>\n    </tr>\n    <tr>\n      <td>438</td>\n      <td>2.705700</td>\n    </tr>\n    <tr>\n      <td>439</td>\n      <td>3.036800</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>2.774900</td>\n    </tr>\n    <tr>\n      <td>441</td>\n      <td>2.415700</td>\n    </tr>\n    <tr>\n      <td>442</td>\n      <td>3.017900</td>\n    </tr>\n    <tr>\n      <td>443</td>\n      <td>2.391300</td>\n    </tr>\n    <tr>\n      <td>444</td>\n      <td>2.701900</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>3.120200</td>\n    </tr>\n    <tr>\n      <td>446</td>\n      <td>2.705300</td>\n    </tr>\n    <tr>\n      <td>447</td>\n      <td>2.892500</td>\n    </tr>\n    <tr>\n      <td>448</td>\n      <td>2.494400</td>\n    </tr>\n    <tr>\n      <td>449</td>\n      <td>2.733900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.899900</td>\n    </tr>\n    <tr>\n      <td>451</td>\n      <td>3.015400</td>\n    </tr>\n    <tr>\n      <td>452</td>\n      <td>2.754600</td>\n    </tr>\n    <tr>\n      <td>453</td>\n      <td>2.785800</td>\n    </tr>\n    <tr>\n      <td>454</td>\n      <td>2.589700</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>2.911700</td>\n    </tr>\n    <tr>\n      <td>456</td>\n      <td>2.847600</td>\n    </tr>\n    <tr>\n      <td>457</td>\n      <td>2.873700</td>\n    </tr>\n    <tr>\n      <td>458</td>\n      <td>2.769000</td>\n    </tr>\n    <tr>\n      <td>459</td>\n      <td>2.869600</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>2.754300</td>\n    </tr>\n    <tr>\n      <td>461</td>\n      <td>2.861200</td>\n    </tr>\n    <tr>\n      <td>462</td>\n      <td>2.852700</td>\n    </tr>\n    <tr>\n      <td>463</td>\n      <td>2.841800</td>\n    </tr>\n    <tr>\n      <td>464</td>\n      <td>3.108300</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>2.937400</td>\n    </tr>\n    <tr>\n      <td>466</td>\n      <td>2.815400</td>\n    </tr>\n    <tr>\n      <td>467</td>\n      <td>2.770000</td>\n    </tr>\n    <tr>\n      <td>468</td>\n      <td>2.687700</td>\n    </tr>\n    <tr>\n      <td>469</td>\n      <td>2.674400</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>2.835800</td>\n    </tr>\n    <tr>\n      <td>471</td>\n      <td>2.726900</td>\n    </tr>\n    <tr>\n      <td>472</td>\n      <td>2.584800</td>\n    </tr>\n    <tr>\n      <td>473</td>\n      <td>2.909300</td>\n    </tr>\n    <tr>\n      <td>474</td>\n      <td>2.523600</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>2.987900</td>\n    </tr>\n    <tr>\n      <td>476</td>\n      <td>2.691200</td>\n    </tr>\n    <tr>\n      <td>477</td>\n      <td>2.847000</td>\n    </tr>\n    <tr>\n      <td>478</td>\n      <td>2.659200</td>\n    </tr>\n    <tr>\n      <td>479</td>\n      <td>3.075100</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>2.827500</td>\n    </tr>\n    <tr>\n      <td>481</td>\n      <td>3.054700</td>\n    </tr>\n    <tr>\n      <td>482</td>\n      <td>2.962600</td>\n    </tr>\n    <tr>\n      <td>483</td>\n      <td>2.815600</td>\n    </tr>\n    <tr>\n      <td>484</td>\n      <td>2.773900</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>2.960200</td>\n    </tr>\n    <tr>\n      <td>486</td>\n      <td>2.416400</td>\n    </tr>\n    <tr>\n      <td>487</td>\n      <td>2.829000</td>\n    </tr>\n    <tr>\n      <td>488</td>\n      <td>2.939900</td>\n    </tr>\n    <tr>\n      <td>489</td>\n      <td>2.918900</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>2.721400</td>\n    </tr>\n    <tr>\n      <td>491</td>\n      <td>2.949200</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>2.557200</td>\n    </tr>\n    <tr>\n      <td>493</td>\n      <td>2.728900</td>\n    </tr>\n    <tr>\n      <td>494</td>\n      <td>2.779800</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>2.910400</td>\n    </tr>\n    <tr>\n      <td>496</td>\n      <td>2.998800</td>\n    </tr>\n    <tr>\n      <td>497</td>\n      <td>2.719800</td>\n    </tr>\n    <tr>\n      <td>498</td>\n      <td>2.656000</td>\n    </tr>\n    <tr>\n      <td>499</td>\n      <td>2.661900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.781000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"gpt2\",\n    device_map='auto',\n)\n\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(\"lora.pt\", map_location=device))\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\nwith torch.no_grad():\n    batch = tokenizer(\"“Life is like a box of chocolates, you never know what you are gonna get” ->: \", return_tensors='pt').to(device)\n    output_tokens = model.generate(**batch, max_new_tokens=25)\n\nprint('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T10:11:47.290409Z","iopub.execute_input":"2024-11-28T10:11:47.293382Z","iopub.status.idle":"2024-11-28T10:11:52.556336Z","shell.execute_reply.started":"2024-11-28T10:11:47.293252Z","shell.execute_reply":"2024-11-28T10:11:52.554016Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n/tmp/ipykernel_298/2227076243.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"lora.pt\", map_location=device))\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\n “Life is like a box of chocolates, you never know what you are gonna get” ->: vernacular, love, life, love-inspirational-life, love-inspirational-life-inspir\n","output_type":"stream"}],"execution_count":10}]}