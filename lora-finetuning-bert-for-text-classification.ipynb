{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch transformers datasets peft accelerate scikit-learn evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:01:48.719377Z","iopub.execute_input":"2024-11-29T07:01:48.719792Z","iopub.status.idle":"2024-11-29T07:01:59.618211Z","shell.execute_reply.started":"2024-11-29T07:01:48.719759Z","shell.execute_reply":"2024-11-29T07:01:59.616381Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoTokenizer, BertForSequenceClassification\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:58:11.022708Z","iopub.execute_input":"2024-11-29T06:58:11.023125Z","iopub.status.idle":"2024-11-29T06:58:11.028506Z","shell.execute_reply.started":"2024-11-29T06:58:11.023092Z","shell.execute_reply":"2024-11-29T06:58:11.027318Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"dataset = load_dataset(\"ag_news\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:53:44.343846Z","iopub.execute_input":"2024-11-29T06:53:44.344220Z","iopub.status.idle":"2024-11-29T06:53:47.060121Z","shell.execute_reply.started":"2024-11-29T06:53:44.344189Z","shell.execute_reply":"2024-11-29T06:53:47.058982Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"face1d05030a40f4863e99c1b1700a52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a74f0a77932405692e89edc7c19573b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f7c26d1855471a9de7aeef2a4cbd7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"062194b4c6094ba4aaaa80822606f761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"381c7b2b1b824b36a4ef5f5937cef0a7"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\nprint(tokenized_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:54:27.371578Z","iopub.execute_input":"2024-11-29T06:54:27.372013Z","iopub.status.idle":"2024-11-29T06:55:15.915577Z","shell.execute_reply.started":"2024-11-29T06:54:27.371978Z","shell.execute_reply":"2024-11-29T06:55:15.914267Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01878a6dc02b4b05843e079b09af1a24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"165bc83223914129b5d54dc14b202141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60d94faee58f491a8d94cab770c8d327"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a26478a65274d8d895ecd900b552a29"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a04440446dfb46b1854ecf670b42ac24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdcd50229cf54f85be9c36b1bcd92aa5"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 120000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 7600\n    })\n})\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(tokenized_datasets[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:56:05.314711Z","iopub.execute_input":"2024-11-29T06:56:05.315108Z","iopub.status.idle":"2024-11-29T06:56:05.322537Z","shell.execute_reply.started":"2024-11-29T06:56:05.315074Z","shell.execute_reply":"2024-11-29T06:56:05.321176Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2, 'input_ids': [101, 6250, 1457, 119, 10169, 140, 9598, 4388, 14000, 1103, 2117, 113, 11336, 27603, 114, 11336, 27603, 118, 6373, 118, 18275, 1116, 117, 6250, 1715, 112, 188, 173, 11129, 1979, 165, 1467, 1104, 18737, 118, 172, 5730, 4724, 117, 1132, 3195, 2448, 1254, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:56:54.615617Z","iopub.execute_input":"2024-11-29T06:56:54.616037Z","iopub.status.idle":"2024-11-29T06:56:54.635620Z","shell.execute_reply.started":"2024-11-29T06:56:54.616002Z","shell.execute_reply":"2024-11-29T06:56:54.634516Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# LoRa\nconfig = LoraConfig(\n    task_type=TaskType.SEQ_CLS,  # Sequence classification\n    inference_mode=False,  # Training mode\n    r=8,  # Low-rank dimension\n    lora_alpha=32,  # Scaling factor for LoRA\n    lora_dropout=0.1  # Dropout for LoRA layers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:58:14.882827Z","iopub.execute_input":"2024-11-29T06:58:14.883361Z","iopub.status.idle":"2024-11-29T06:58:14.890141Z","shell.execute_reply.started":"2024-11-29T06:58:14.883310Z","shell.execute_reply":"2024-11-29T06:58:14.888880Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    'bert-base-cased', \n    num_labels=4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:58:31.105061Z","iopub.execute_input":"2024-11-29T06:58:31.105481Z","iopub.status.idle":"2024-11-29T06:58:33.629608Z","shell.execute_reply.started":"2024-11-29T06:58:31.105446Z","shell.execute_reply":"2024-11-29T06:58:33.628345Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef874bb701ac415eac562cc28a73dd46"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"model = get_peft_model(model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:58:59.700005Z","iopub.execute_input":"2024-11-29T06:58:59.700446Z","iopub.status.idle":"2024-11-29T06:58:59.755802Z","shell.execute_reply.started":"2024-11-29T06:58:59.700408Z","shell.execute_reply":"2024-11-29T06:58:59.754759Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:00:19.814807Z","iopub.execute_input":"2024-11-29T07:00:19.815396Z","iopub.status.idle":"2024-11-29T07:00:19.826521Z","shell.execute_reply.started":"2024-11-29T07:00:19.815341Z","shell.execute_reply":"2024-11-29T07:00:19.825060Z"}},"outputs":[{"name":"stdout","text":"trainable params: 297,988 || all params: 108,611,336 || trainable%: 0.2744\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:01:59.620793Z","iopub.execute_input":"2024-11-29T07:01:59.621178Z","iopub.status.idle":"2024-11-29T07:02:15.577471Z","shell.execute_reply.started":"2024-11-29T07:01:59.621138Z","shell.execute_reply":"2024-11-29T07:02:15.576186Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92c421248b24ad5bb77cb1c26bff222"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"outputs\", \n    max_steps=500,\n    learning_rate=2e-4,\n    auto_find_batch_size=True,\n    logging_steps=1\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics    \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:45:44.434344Z","iopub.execute_input":"2024-11-29T07:45:44.434769Z","iopub.status.idle":"2024-11-29T07:45:44.461551Z","shell.execute_reply.started":"2024-11-29T07:45:44.434736Z","shell.execute_reply":"2024-11-29T07:45:44.460358Z"}},"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T07:45:46.819253Z","iopub.execute_input":"2024-11-29T07:45:46.819731Z","iopub.status.idle":"2024-11-29T10:04:08.883685Z","shell.execute_reply.started":"2024-11-29T07:45:46.819693Z","shell.execute_reply":"2024-11-29T10:04:08.882083Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 2:18:04, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.019700</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.926900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.762800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.766700</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.712500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.266400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.925800</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.654100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.829100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.898400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.292600</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.895000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.561400</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.261700</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.906000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.204700</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.777300</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.674200</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.814000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.719800</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.737900</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.030900</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.803600</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.778300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.602200</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.901400</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.776500</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.966900</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.793100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.645600</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.095700</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.900800</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.639700</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.526200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.618700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.242000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.473700</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.788500</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.571200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.494300</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.756500</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.787900</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.494600</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.497000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.464600</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.599500</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.535000</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.604100</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.687500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.729000</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.475500</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.348700</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.852400</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.312300</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.435800</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.621800</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.317000</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.721100</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.252100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.524300</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.635700</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.254100</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.196300</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.313300</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.824800</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.335000</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.515400</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.259500</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.204400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.373300</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.279800</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.128500</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.187000</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.339300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.176000</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.666400</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.484600</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.120200</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.262900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.362600</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>1.095400</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.147700</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.245800</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.189000</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.733600</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.206900</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.323800</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.299200</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.713100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.127500</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>1.140800</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>1.228400</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>1.203400</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.466800</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.501600</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.526800</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.962500</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.235800</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.837100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.249800</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>0.698400</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>0.653700</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>1.336900</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>1.033000</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.248000</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>0.306200</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>0.176800</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>0.213600</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>0.284400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.978900</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>0.085500</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>1.325700</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>0.773500</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>0.533700</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.987500</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>0.781800</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>0.643800</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>0.141900</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>0.634100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.099700</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>0.803700</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>1.820100</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>0.508200</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>1.635000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.492800</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>0.730300</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>0.511400</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>0.870700</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>0.505100</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.312600</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>0.129300</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>0.076900</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>0.366200</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>0.339900</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.162000</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>0.285100</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>0.080800</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>0.689000</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>0.972500</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.658100</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.220100</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>0.244700</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>1.705300</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>0.168600</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.270100</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>0.278700</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>0.429600</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>0.575300</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>0.999000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.789400</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>0.123000</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>0.103900</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>0.467400</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>0.455800</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1.104900</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.455400</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>0.167800</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>0.147200</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>0.379800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.259200</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>0.189600</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>0.537600</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>0.222000</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>0.884000</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.147100</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>0.747700</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>0.238900</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>0.707600</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>1.408700</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.194200</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>0.441300</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>0.238300</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>0.221600</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>0.140800</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.150100</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>0.195700</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>0.165100</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>0.687900</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>0.371400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.520200</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>0.754500</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>0.167700</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>0.042900</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>0.070000</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.110000</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>0.949900</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>0.084200</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>0.196800</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>0.050600</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.283100</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>0.156700</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>0.441700</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>0.043300</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>0.665700</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.072100</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>0.080300</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>1.276000</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>0.151100</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>1.392700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.316600</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>0.098500</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>0.060200</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>1.220700</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>0.046100</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.077400</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>0.797900</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>0.097200</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>0.772100</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>0.437500</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.070800</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>1.004900</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>0.285700</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>0.636300</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>0.582300</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.568700</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>0.060300</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>0.448300</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>0.040100</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>0.961600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.434200</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>0.048900</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>0.318200</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>0.734900</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>0.284600</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.404400</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>0.644200</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>0.295200</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>0.793300</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>0.205500</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.235800</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>0.596600</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>0.041600</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>0.374300</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>0.410800</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.170500</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>0.026400</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>0.057300</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>1.053800</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>0.379900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.406300</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>1.486900</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>0.286300</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>0.585500</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>0.039400</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.803800</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>0.202000</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>0.430300</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>0.325400</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>0.606000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.616400</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>0.577200</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>0.247300</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>0.079500</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>0.047800</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.041500</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>0.588500</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>0.052700</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>0.307200</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>0.056500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.092900</td>\n    </tr>\n    <tr>\n      <td>261</td>\n      <td>0.448900</td>\n    </tr>\n    <tr>\n      <td>262</td>\n      <td>0.158400</td>\n    </tr>\n    <tr>\n      <td>263</td>\n      <td>0.037800</td>\n    </tr>\n    <tr>\n      <td>264</td>\n      <td>0.201300</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.636000</td>\n    </tr>\n    <tr>\n      <td>266</td>\n      <td>0.335500</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>0.351500</td>\n    </tr>\n    <tr>\n      <td>268</td>\n      <td>0.758000</td>\n    </tr>\n    <tr>\n      <td>269</td>\n      <td>0.361900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.059900</td>\n    </tr>\n    <tr>\n      <td>271</td>\n      <td>0.602600</td>\n    </tr>\n    <tr>\n      <td>272</td>\n      <td>0.372500</td>\n    </tr>\n    <tr>\n      <td>273</td>\n      <td>0.163900</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>0.850000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.380400</td>\n    </tr>\n    <tr>\n      <td>276</td>\n      <td>0.106400</td>\n    </tr>\n    <tr>\n      <td>277</td>\n      <td>0.065400</td>\n    </tr>\n    <tr>\n      <td>278</td>\n      <td>0.054700</td>\n    </tr>\n    <tr>\n      <td>279</td>\n      <td>1.078500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.647500</td>\n    </tr>\n    <tr>\n      <td>281</td>\n      <td>0.127500</td>\n    </tr>\n    <tr>\n      <td>282</td>\n      <td>0.132800</td>\n    </tr>\n    <tr>\n      <td>283</td>\n      <td>0.107200</td>\n    </tr>\n    <tr>\n      <td>284</td>\n      <td>0.072700</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.442600</td>\n    </tr>\n    <tr>\n      <td>286</td>\n      <td>0.447000</td>\n    </tr>\n    <tr>\n      <td>287</td>\n      <td>0.046800</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>0.067100</td>\n    </tr>\n    <tr>\n      <td>289</td>\n      <td>0.151900</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.821200</td>\n    </tr>\n    <tr>\n      <td>291</td>\n      <td>0.230600</td>\n    </tr>\n    <tr>\n      <td>292</td>\n      <td>1.275800</td>\n    </tr>\n    <tr>\n      <td>293</td>\n      <td>0.029800</td>\n    </tr>\n    <tr>\n      <td>294</td>\n      <td>0.444100</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.124500</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>0.393100</td>\n    </tr>\n    <tr>\n      <td>297</td>\n      <td>1.432300</td>\n    </tr>\n    <tr>\n      <td>298</td>\n      <td>0.424400</td>\n    </tr>\n    <tr>\n      <td>299</td>\n      <td>0.696000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.422700</td>\n    </tr>\n    <tr>\n      <td>301</td>\n      <td>1.137000</td>\n    </tr>\n    <tr>\n      <td>302</td>\n      <td>0.050100</td>\n    </tr>\n    <tr>\n      <td>303</td>\n      <td>0.053000</td>\n    </tr>\n    <tr>\n      <td>304</td>\n      <td>0.111400</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>0.208900</td>\n    </tr>\n    <tr>\n      <td>306</td>\n      <td>0.674900</td>\n    </tr>\n    <tr>\n      <td>307</td>\n      <td>1.029000</td>\n    </tr>\n    <tr>\n      <td>308</td>\n      <td>0.261100</td>\n    </tr>\n    <tr>\n      <td>309</td>\n      <td>0.058300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.260400</td>\n    </tr>\n    <tr>\n      <td>311</td>\n      <td>0.062300</td>\n    </tr>\n    <tr>\n      <td>312</td>\n      <td>0.099000</td>\n    </tr>\n    <tr>\n      <td>313</td>\n      <td>0.738500</td>\n    </tr>\n    <tr>\n      <td>314</td>\n      <td>0.047300</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>0.173600</td>\n    </tr>\n    <tr>\n      <td>316</td>\n      <td>0.071500</td>\n    </tr>\n    <tr>\n      <td>317</td>\n      <td>0.836700</td>\n    </tr>\n    <tr>\n      <td>318</td>\n      <td>0.646400</td>\n    </tr>\n    <tr>\n      <td>319</td>\n      <td>0.891600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.865800</td>\n    </tr>\n    <tr>\n      <td>321</td>\n      <td>1.020100</td>\n    </tr>\n    <tr>\n      <td>322</td>\n      <td>0.034500</td>\n    </tr>\n    <tr>\n      <td>323</td>\n      <td>0.163800</td>\n    </tr>\n    <tr>\n      <td>324</td>\n      <td>0.054900</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.061500</td>\n    </tr>\n    <tr>\n      <td>326</td>\n      <td>0.667900</td>\n    </tr>\n    <tr>\n      <td>327</td>\n      <td>0.523900</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>1.051300</td>\n    </tr>\n    <tr>\n      <td>329</td>\n      <td>0.112200</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.112500</td>\n    </tr>\n    <tr>\n      <td>331</td>\n      <td>0.038100</td>\n    </tr>\n    <tr>\n      <td>332</td>\n      <td>0.049300</td>\n    </tr>\n    <tr>\n      <td>333</td>\n      <td>0.112700</td>\n    </tr>\n    <tr>\n      <td>334</td>\n      <td>0.507000</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <td>336</td>\n      <td>0.269200</td>\n    </tr>\n    <tr>\n      <td>337</td>\n      <td>0.023400</td>\n    </tr>\n    <tr>\n      <td>338</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>339</td>\n      <td>0.922000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.042700</td>\n    </tr>\n    <tr>\n      <td>341</td>\n      <td>0.088900</td>\n    </tr>\n    <tr>\n      <td>342</td>\n      <td>0.055900</td>\n    </tr>\n    <tr>\n      <td>343</td>\n      <td>0.154100</td>\n    </tr>\n    <tr>\n      <td>344</td>\n      <td>0.438800</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>0.716600</td>\n    </tr>\n    <tr>\n      <td>346</td>\n      <td>0.800200</td>\n    </tr>\n    <tr>\n      <td>347</td>\n      <td>0.218000</td>\n    </tr>\n    <tr>\n      <td>348</td>\n      <td>0.049200</td>\n    </tr>\n    <tr>\n      <td>349</td>\n      <td>0.310700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.063700</td>\n    </tr>\n    <tr>\n      <td>351</td>\n      <td>0.044800</td>\n    </tr>\n    <tr>\n      <td>352</td>\n      <td>0.396600</td>\n    </tr>\n    <tr>\n      <td>353</td>\n      <td>0.282100</td>\n    </tr>\n    <tr>\n      <td>354</td>\n      <td>0.571500</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>0.194400</td>\n    </tr>\n    <tr>\n      <td>356</td>\n      <td>0.215300</td>\n    </tr>\n    <tr>\n      <td>357</td>\n      <td>0.819200</td>\n    </tr>\n    <tr>\n      <td>358</td>\n      <td>0.029400</td>\n    </tr>\n    <tr>\n      <td>359</td>\n      <td>0.383900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.121200</td>\n    </tr>\n    <tr>\n      <td>361</td>\n      <td>0.442200</td>\n    </tr>\n    <tr>\n      <td>362</td>\n      <td>0.146800</td>\n    </tr>\n    <tr>\n      <td>363</td>\n      <td>0.080200</td>\n    </tr>\n    <tr>\n      <td>364</td>\n      <td>0.048400</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>0.158900</td>\n    </tr>\n    <tr>\n      <td>366</td>\n      <td>0.054300</td>\n    </tr>\n    <tr>\n      <td>367</td>\n      <td>1.161500</td>\n    </tr>\n    <tr>\n      <td>368</td>\n      <td>0.092000</td>\n    </tr>\n    <tr>\n      <td>369</td>\n      <td>0.235300</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.075900</td>\n    </tr>\n    <tr>\n      <td>371</td>\n      <td>0.338700</td>\n    </tr>\n    <tr>\n      <td>372</td>\n      <td>0.356000</td>\n    </tr>\n    <tr>\n      <td>373</td>\n      <td>0.036300</td>\n    </tr>\n    <tr>\n      <td>374</td>\n      <td>0.258300</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.303200</td>\n    </tr>\n    <tr>\n      <td>376</td>\n      <td>1.239600</td>\n    </tr>\n    <tr>\n      <td>377</td>\n      <td>0.293300</td>\n    </tr>\n    <tr>\n      <td>378</td>\n      <td>0.023400</td>\n    </tr>\n    <tr>\n      <td>379</td>\n      <td>0.051100</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.210100</td>\n    </tr>\n    <tr>\n      <td>381</td>\n      <td>0.442700</td>\n    </tr>\n    <tr>\n      <td>382</td>\n      <td>0.330100</td>\n    </tr>\n    <tr>\n      <td>383</td>\n      <td>0.074800</td>\n    </tr>\n    <tr>\n      <td>384</td>\n      <td>0.109900</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>0.050700</td>\n    </tr>\n    <tr>\n      <td>386</td>\n      <td>0.087900</td>\n    </tr>\n    <tr>\n      <td>387</td>\n      <td>0.086600</td>\n    </tr>\n    <tr>\n      <td>388</td>\n      <td>0.038100</td>\n    </tr>\n    <tr>\n      <td>389</td>\n      <td>0.024400</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.141400</td>\n    </tr>\n    <tr>\n      <td>391</td>\n      <td>0.394300</td>\n    </tr>\n    <tr>\n      <td>392</td>\n      <td>0.300500</td>\n    </tr>\n    <tr>\n      <td>393</td>\n      <td>0.108900</td>\n    </tr>\n    <tr>\n      <td>394</td>\n      <td>0.845600</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>0.426700</td>\n    </tr>\n    <tr>\n      <td>396</td>\n      <td>0.530500</td>\n    </tr>\n    <tr>\n      <td>397</td>\n      <td>0.517200</td>\n    </tr>\n    <tr>\n      <td>398</td>\n      <td>0.129400</td>\n    </tr>\n    <tr>\n      <td>399</td>\n      <td>0.143300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.039000</td>\n    </tr>\n    <tr>\n      <td>401</td>\n      <td>0.063700</td>\n    </tr>\n    <tr>\n      <td>402</td>\n      <td>0.075600</td>\n    </tr>\n    <tr>\n      <td>403</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>0.032700</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>0.190500</td>\n    </tr>\n    <tr>\n      <td>406</td>\n      <td>0.023700</td>\n    </tr>\n    <tr>\n      <td>407</td>\n      <td>1.248600</td>\n    </tr>\n    <tr>\n      <td>408</td>\n      <td>0.418300</td>\n    </tr>\n    <tr>\n      <td>409</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.468400</td>\n    </tr>\n    <tr>\n      <td>411</td>\n      <td>0.805100</td>\n    </tr>\n    <tr>\n      <td>412</td>\n      <td>1.232600</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>0.494200</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>0.139100</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>0.090800</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>0.029600</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>0.410600</td>\n    </tr>\n    <tr>\n      <td>418</td>\n      <td>0.479900</td>\n    </tr>\n    <tr>\n      <td>419</td>\n      <td>0.180800</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.042000</td>\n    </tr>\n    <tr>\n      <td>421</td>\n      <td>0.689700</td>\n    </tr>\n    <tr>\n      <td>422</td>\n      <td>0.589600</td>\n    </tr>\n    <tr>\n      <td>423</td>\n      <td>0.460200</td>\n    </tr>\n    <tr>\n      <td>424</td>\n      <td>0.028500</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.635000</td>\n    </tr>\n    <tr>\n      <td>426</td>\n      <td>0.940700</td>\n    </tr>\n    <tr>\n      <td>427</td>\n      <td>0.291700</td>\n    </tr>\n    <tr>\n      <td>428</td>\n      <td>0.318100</td>\n    </tr>\n    <tr>\n      <td>429</td>\n      <td>1.480800</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.460600</td>\n    </tr>\n    <tr>\n      <td>431</td>\n      <td>0.512500</td>\n    </tr>\n    <tr>\n      <td>432</td>\n      <td>1.107000</td>\n    </tr>\n    <tr>\n      <td>433</td>\n      <td>0.152700</td>\n    </tr>\n    <tr>\n      <td>434</td>\n      <td>1.042200</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>0.040000</td>\n    </tr>\n    <tr>\n      <td>436</td>\n      <td>0.030400</td>\n    </tr>\n    <tr>\n      <td>437</td>\n      <td>0.205000</td>\n    </tr>\n    <tr>\n      <td>438</td>\n      <td>0.416300</td>\n    </tr>\n    <tr>\n      <td>439</td>\n      <td>1.342900</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.040700</td>\n    </tr>\n    <tr>\n      <td>441</td>\n      <td>0.041300</td>\n    </tr>\n    <tr>\n      <td>442</td>\n      <td>0.277700</td>\n    </tr>\n    <tr>\n      <td>443</td>\n      <td>0.242600</td>\n    </tr>\n    <tr>\n      <td>444</td>\n      <td>0.872000</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>0.126900</td>\n    </tr>\n    <tr>\n      <td>446</td>\n      <td>0.249100</td>\n    </tr>\n    <tr>\n      <td>447</td>\n      <td>0.685900</td>\n    </tr>\n    <tr>\n      <td>448</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>449</td>\n      <td>0.193600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.032300</td>\n    </tr>\n    <tr>\n      <td>451</td>\n      <td>0.773700</td>\n    </tr>\n    <tr>\n      <td>452</td>\n      <td>0.028600</td>\n    </tr>\n    <tr>\n      <td>453</td>\n      <td>0.135700</td>\n    </tr>\n    <tr>\n      <td>454</td>\n      <td>0.024400</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>0.459800</td>\n    </tr>\n    <tr>\n      <td>456</td>\n      <td>0.240100</td>\n    </tr>\n    <tr>\n      <td>457</td>\n      <td>0.219900</td>\n    </tr>\n    <tr>\n      <td>458</td>\n      <td>0.036700</td>\n    </tr>\n    <tr>\n      <td>459</td>\n      <td>0.714200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.030400</td>\n    </tr>\n    <tr>\n      <td>461</td>\n      <td>0.519500</td>\n    </tr>\n    <tr>\n      <td>462</td>\n      <td>0.103000</td>\n    </tr>\n    <tr>\n      <td>463</td>\n      <td>0.246500</td>\n    </tr>\n    <tr>\n      <td>464</td>\n      <td>0.560800</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>0.045600</td>\n    </tr>\n    <tr>\n      <td>466</td>\n      <td>0.742100</td>\n    </tr>\n    <tr>\n      <td>467</td>\n      <td>0.084200</td>\n    </tr>\n    <tr>\n      <td>468</td>\n      <td>0.030600</td>\n    </tr>\n    <tr>\n      <td>469</td>\n      <td>1.758500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.153400</td>\n    </tr>\n    <tr>\n      <td>471</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>472</td>\n      <td>0.023200</td>\n    </tr>\n    <tr>\n      <td>473</td>\n      <td>0.668100</td>\n    </tr>\n    <tr>\n      <td>474</td>\n      <td>0.034800</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.036000</td>\n    </tr>\n    <tr>\n      <td>476</td>\n      <td>0.306200</td>\n    </tr>\n    <tr>\n      <td>477</td>\n      <td>0.428800</td>\n    </tr>\n    <tr>\n      <td>478</td>\n      <td>0.054700</td>\n    </tr>\n    <tr>\n      <td>479</td>\n      <td>0.034900</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.048500</td>\n    </tr>\n    <tr>\n      <td>481</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>482</td>\n      <td>0.410600</td>\n    </tr>\n    <tr>\n      <td>483</td>\n      <td>0.359400</td>\n    </tr>\n    <tr>\n      <td>484</td>\n      <td>0.310600</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>486</td>\n      <td>0.093000</td>\n    </tr>\n    <tr>\n      <td>487</td>\n      <td>0.379300</td>\n    </tr>\n    <tr>\n      <td>488</td>\n      <td>0.920400</td>\n    </tr>\n    <tr>\n      <td>489</td>\n      <td>0.468600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>491</td>\n      <td>0.111900</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>0.286100</td>\n    </tr>\n    <tr>\n      <td>493</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <td>494</td>\n      <td>0.033900</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>0.571900</td>\n    </tr>\n    <tr>\n      <td>496</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>497</td>\n      <td>0.445200</td>\n    </tr>\n    <tr>\n      <td>498</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>499</td>\n      <td>0.021200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.756100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=0.4429442072436213, metrics={'train_runtime': 8300.398, 'train_samples_per_second': 0.482, 'train_steps_per_second': 0.06, 'total_flos': 1056124796928000.0, 'train_loss': 0.4429442072436213, 'epoch': 4.0})"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"torch.save(model.state_dict(), 'lora_bert_classifier.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T10:04:08.887139Z","iopub.execute_input":"2024-11-29T10:04:08.887572Z","iopub.status.idle":"2024-11-29T10:04:10.322310Z","shell.execute_reply.started":"2024-11-29T10:04:08.887535Z","shell.execute_reply":"2024-11-29T10:04:10.320900Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoConfig, BertForSequenceClassification\nfrom peft import LoraConfig, get_peft_model\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-cased', \n    device_map='auto'\n)\n\n# model = BertForSequenceClassification.from_pretrained(\n#     \"gpt2\",\n#     device_map='auto',\n# )\n\nconfig = LoraConfig(\n    task_type=TaskType.SEQ_CLS,  # Sequence classification\n    inference_mode=False,  # Training mode\n    r=8,  # Low-rank dimension\n    lora_alpha=32,  # Scaling factor for LoRA\n    lora_dropout=0.1  # Dropout for LoRA layers\n)\n\nmodel = get_peft_model(model, config)\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(\"lora_bert_classifier.pt\", map_location=device))\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n# tokenizer.pad_token = tokenizer.eos_token\n\nwith torch.no_grad():\n    batch = tokenizer(\"“Life is like a box of chocolates, you never know what you are gonna get” ->: \", return_tensors='pt').to(device)\n    output_tokens = model.generate(**batch, max_new_tokens=15)\n\nprint('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:33:43.869643Z","iopub.execute_input":"2024-11-30T06:33:43.870082Z","iopub.status.idle":"2024-11-30T06:33:49.911670Z","shell.execute_reply.started":"2024-11-30T06:33:43.870010Z","shell.execute_reply":"2024-11-30T06:33:49.910114Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoConfig, BertForSequenceClassification\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      9\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"],"ename":"ModuleNotFoundError","evalue":"No module named 'peft'","output_type":"error"}],"execution_count":1}]}